# **Reinforcement Learning-Based Compensation for UR5e Robot Control**

## **1. Project Overview**
This project implements a reinforcement learning-based compensation method for controlling the UR5e robot to achieve high tracking accuracy in following a reference trajectory, such as a square path. We employ two prominent reinforcement learning algorithms: **Proximal Policy Optimization (PPO)** and **Soft Actor-Critic (SAC)**, to enhance the robot's performance.

In this setup:
- **Inverse kinematics** is used to calculate the desired joint positions for each point on the trajectory.
- A **Proportional-Derivative (PD) controller** computes the control signals.
- The **reinforcement learning compensator** adjusts the reference signals based on real-time errors to improve tracking accuracy.
- A **low-pass filter** is applied to the actions generated by the RL agent to mitigate high-frequency fluctuations and ensure stable control.

The focus of this project is to simulate the UR5e robot using RL-based compensators and evaluate their performance in trajectory tracking tasks.

The block diagram of the project is as follows:
![Object Detection](Blockdiagram.jpg)
