{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "from gymnasium import spaces\n",
    "from ray import tune\n",
    "import ray\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "from math import sqrt, pi, cos, sin, atan2, acos, asin\n",
    "from numpy import linalg\n",
    "import cmath\n",
    "\n",
    "# ----------------------------\n",
    "# Configure Logging\n",
    "# ----------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to INFO to reduce logs\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"training.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ----------------------------\n",
    "# Custom UR5e Robot Control\n",
    "# ----------------------------\n",
    "class CustomUR5eRobot:\n",
    "\n",
    "    def __init__(self, urdf_path: str):\n",
    "        self.urdf_path = urdf_path\n",
    "        self.robot_id = None\n",
    "        self.joint_indices = []\n",
    "        self.n_joints = 6  # Assuming UR5e has 6 controllable joints\n",
    "        self.filtered_action = np.zeros(self.n_joints)\n",
    "        self.prev_position_error = np.zeros(self.n_joints)\n",
    "        self.delta_deg_prev = np.zeros(self.n_joints)\n",
    "    def load_robot(self):\n",
    "        \"\"\"\n",
    "        Loads the UR5e robot into the PyBullet simulation.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.urdf_path):\n",
    "            logger.error(f\"URDF file {self.urdf_path} does not exist.\")\n",
    "            raise FileNotFoundError(f\"URDF file {self.urdf_path} does not exist.\")\n",
    "        self.robot_id = p.loadURDF(self.urdf_path, [0, 0, 0], useFixedBase=True)\n",
    "        # Assume the first 6 revolute joints are controllable\n",
    "        self.joint_indices = [\n",
    "            i for i in range(p.getNumJoints(self.robot_id))\n",
    "            if p.getJointInfo(self.robot_id, i)[2] == p.JOINT_REVOLUTE\n",
    "        ][:self.n_joints]\n",
    "        if len(self.joint_indices) < self.n_joints:\n",
    "            logger.error(f\"Expected {self.n_joints} controllable joints, found {len(self.joint_indices)}.\")\n",
    "            raise ValueError(f\"Expected {self.n_joints} controllable joints, found {len(self.joint_indices)}.\")\n",
    "        logger.info(f\"Loaded robot with ID {self.robot_id}. Joint indices: {self.joint_indices}\")\n",
    "\n",
    "    def reset_joints(self, initial_positions: np.ndarray, initial_velocities: np.ndarray):\n",
    "        \"\"\"\n",
    "        Resets the robot's joint positions and velocities.\n",
    "        \"\"\"\n",
    "        for i, joint_idx in enumerate(self.joint_indices):\n",
    "            p.resetJointState(self.robot_id, joint_idx, targetValue=initial_positions[i], targetVelocity=initial_velocities[i])\n",
    "            # Disable default motor control\n",
    "            p.setJointMotorControl2(\n",
    "                bodyIndex=self.robot_id,\n",
    "                jointIndex=joint_idx,\n",
    "                controlMode=p.VELOCITY_CONTROL,\n",
    "                targetVelocity=0,\n",
    "                force=0\n",
    "            )\n",
    "        logger.info(\"Reset joints to initial positions and velocities.\")\n",
    "\n",
    "\n",
    "    def AH(self,n, th, c):\n",
    "\n",
    "        mat = np.matrix\n",
    "        d = mat([0.1625, 0, 0, 0.133, 0.0997, 0.101])\n",
    "        alph = mat([pi/2, 0, 0, pi/2, -pi/2, 0])\n",
    "\n",
    "        a = np.array([0, -0.425, -0.39225, 0, 0, 0])\n",
    "        T_a = mat(np.identity(4), copy=False,dtype = np.float32)\n",
    "        T_a[0, 3] = a[n-1]\n",
    "        T_d = mat(np.identity(4), copy=False)\n",
    "        T_d[2, 3] = d[0, n-1]\n",
    "\n",
    "        Rzt = mat([[cos(th[n-1, c]), -sin(th[n-1, c]), 0, 0],\n",
    "                [sin(th[n-1, c]), cos(th[n-1, c]), 0, 0],\n",
    "                [0, 0, 1, 0],\n",
    "                [0, 0, 0, 1]], copy=False)\n",
    "\n",
    "        Rxa = mat([[1, 0, 0, 0],\n",
    "                [0, cos(alph[0, n-1]), -sin(alph[0, n-1]), 0],\n",
    "                [0, sin(alph[0, n-1]), cos(alph[0, n-1]), 0],\n",
    "                [0, 0, 0, 1]], copy=False)\n",
    "\n",
    "        A_i = T_d * Rzt * T_a * Rxa\n",
    "\n",
    "        return A_i\n",
    "\n",
    "    def HTrans(self,th, c):\n",
    "        A_1 = self.AH(1, th, c)\n",
    "        A_2 = self.AH(2, th, c)\n",
    "        A_3 = self.AH(3, th, c)\n",
    "        A_4 = self.AH(4, th, c)\n",
    "        A_5 = self.AH(5, th, c)\n",
    "        A_6 = self.AH(6, th, c)\n",
    "\n",
    "        T_06 = A_1 * A_2 * A_3 * A_4 * A_5 * A_6\n",
    "\n",
    "        return T_06\n",
    "    \n",
    "    def invKine(self,xyz):  # T60\n",
    "        mat = np.matrix\n",
    "        a2 = -0.425\n",
    "        a3 = -0.39225\n",
    "        d4 = 0.133\n",
    "        d6 = 0.101\n",
    "\n",
    "\n",
    "        desired_pos = np.array([[1, 0, 0, xyz[0]], [0, -1, 0, xyz[1]], [0, 0, -1, xyz[2]], [0, 0, 0, 1]],dtype = np.float32)\n",
    "        th = mat(np.zeros((6, 8)))\n",
    "        P_05 = (desired_pos * mat([0, 0, -d6, 1]).T - mat([0, 0, 0, 1]).T)\n",
    "\n",
    "        # print(\"P_05:\", P_05)\n",
    "\n",
    "        # **** theta1 ****\n",
    "\n",
    "        psi = atan2(P_05[1, 0], P_05[0, 0])\n",
    "        phi = float(np.arccos(float(d4) / float(np.sqrt( P_05[1, 0]**2 + P_05[0, 0]**2,dtype = np.float32))))\n",
    "        # The two solutions for theta1 correspond to the shoulder being either left or right\n",
    "        th[0, 0:4] = pi/2 + psi + phi\n",
    "        th[0, 4:8] = pi/2 + psi - phi\n",
    "        th = th.real\n",
    "\n",
    "        # print(\"theta1:\", th[0])\n",
    "\n",
    "        # **** theta5 ****\n",
    "\n",
    "        cl = [0, 4]  # wrist up or down\n",
    "        for i in range(0, len(cl)):\n",
    "            c = cl[i]\n",
    "            T_10 = np.linalg.inv(self.AH(1, th, c))\n",
    "            T_16 = T_10 * desired_pos\n",
    "            #print(T_16)\n",
    "            th[4, c:c+2] = + acos((T_16[2, 3] - d4) / d6)\n",
    "            th[4, c+2:c+4] = - acos((T_16[2, 3] - d4) / d6)\n",
    "\n",
    "        th = th.real\n",
    "\n",
    "        # print(\"theta5:\", th[4])\n",
    "\n",
    "        # **** theta6 ****\n",
    "        # theta6 is not well-defined when sin(theta5) = 0 or when T16(1, 3), T16(2, 3) = 0.\n",
    "\n",
    "        cl = [0, 2, 4, 6]\n",
    "        for i in range(0, len(cl)):\n",
    "            c = cl[i]\n",
    "            T_10 = linalg.inv(self.AH(1, th, c))\n",
    "            T_16 = linalg.inv(T_10 * desired_pos)\n",
    "            th[5, c:c+2] = atan2((-T_16[1, 2] / sin(th[4, c])), (T_16[0, 2] / sin(th[4, c])))\n",
    "\n",
    "        th = th.real\n",
    "\n",
    "        # print(\"theta6:\", th[5])\n",
    "\n",
    "        # **** theta3 ****\n",
    "        cl = [0, 2, 4, 6]\n",
    "        for i in range(0, len(cl)):\n",
    "            c = cl[i]\n",
    "            T_10 = linalg.inv(self.AH(1, th, c))\n",
    "            T_65 = self.AH(6, th, c)\n",
    "            T_54 = self.AH(5, th, c)\n",
    "            T_14 = (T_10 * desired_pos) * linalg.inv(T_54 * T_65)\n",
    "            P_13 = T_14 * mat([0, -d4, 0, 1]).T - mat([0, 0, 0, 1]).T\n",
    "            t3 = cmath.acos((linalg.norm(P_13)**2 - a2**2 - a3**2) / (2 * a2 * a3))  # norm ?\n",
    "            th[2, c] = t3.real\n",
    "            th[2, c+1] = -t3.real\n",
    "\n",
    "        # print(\"theta3:\", th[2])\n",
    "\n",
    "        # **** theta2 and theta 4 ****\n",
    "\n",
    "        cl = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "        for i in range(0, len(cl)):\n",
    "            c = cl[i]\n",
    "            T_10 = np.linalg.inv(self.AH(1, th, c))\n",
    "            T_65 = np.linalg.inv(self.AH(6, th, c))\n",
    "            T_54 = np.linalg.inv(self.AH(5, th, c))\n",
    "            T_14 = (T_10 * desired_pos) * T_65 * T_54\n",
    "            P_13 = T_14 * mat([0, -d4, 0, 1]).T - mat([0, 0, 0, 1]).T\n",
    "\n",
    "            # theta 2\n",
    "            th[1, c] = -atan2(P_13[1], -P_13[0]) + asin(a3 * sin(th[2, c]) / linalg.norm(P_13))\n",
    "            # theta 4\n",
    "            T_32 = linalg.inv(self.AH(3, th, c))\n",
    "            T_21 = linalg.inv(self.AH(2, th, c))\n",
    "            T_34 = T_32 * T_21 * T_14\n",
    "            th[3, c] = atan2(T_34[1, 0], T_34[0, 0])\n",
    "\n",
    "        th = th.real\n",
    "        ik_results = th.T\n",
    "        pos = np.take(ik_results, indices=[2], axis=0)\n",
    "        pos = pos.tolist()\n",
    "\n",
    "\n",
    "        return np.array(pos[0])\n",
    "    def FK(self, joint_angles):\n",
    "        \"\"\"\n",
    "        Computes the forward kinematics for the UR5e robot.\n",
    "\n",
    "        :param joint_angles: Array of 6 joint angles [theta1, theta2, ..., theta6]\n",
    "        :return: Tuple of (x, y, z) positions\n",
    "        \"\"\"\n",
    "        t1, t2, t3, t4, t5, t6 = joint_angles\n",
    "        # Define your robot's specific parameters here\n",
    "        # Example placeholder values (replace with actual UR5e parameters)\n",
    "        d1 = 0.1625\n",
    "        a2 = -0.425\n",
    "        a3 = -0.39225\n",
    "        d4 = 0.133\n",
    "        d5 = 0.0997\n",
    "        d6 = 0.101\n",
    "        x = -(np.cos(t1)*np.cos(t4+t2+t3)*np.sin(t5)*d6) + (np.cos(t5)*np.sin(t1)*d6) + \\\n",
    "            (np.cos(t1)*np.sin(t2+t3+t4)*d5) + (np.sin(t1)*d4) + \\\n",
    "            (np.cos(t1)*np.cos(t2+t3)*a3) + (np.cos(t1)*np.cos(t2)*a2)\n",
    "        y = -(np.cos(t2+t3+t4)*np.sin(t1)*np.sin(t5)*d6) - (np.cos(t1)*np.cos(t5)*d6) + \\\n",
    "            (np.sin(t1)*np.sin(t2+t3+t4)*d5) - (np.cos(t1)*d4) + \\\n",
    "            (np.cos(t2+t3)*np.sin(t1)*a3) + (np.cos(t2)*np.sin(t1)*a2)\n",
    "        z = -(np.sin(t2+t3+t4)*np.sin(t5)*d6) - (np.cos(t2+t3+t4)*d5) + \\\n",
    "            (np.sin(t2+t3)*a3) + (np.sin(t2)*a2) + d1\n",
    "\n",
    "        # Check for numerical issues\n",
    "        if any(np.isnan([x, y, z])) or any(np.isinf([x, y, z])):\n",
    "            logger.error(f\"FK computation resulted in invalid values: x={x}, y={y}, z={z}\")\n",
    "            return 0.0, 0.0, 0.0  # Default fallback values\n",
    "\n",
    "        return x, y, z\n",
    "    \n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "        Retrieves the current joint positions and velocities.\n",
    "        \"\"\"\n",
    "        joint_positions = []\n",
    "        joint_velocities = []\n",
    "        for joint_idx in self.joint_indices:\n",
    "            joint_state = p.getJointState(self.robot_id, joint_idx)\n",
    "            joint_positions.append(joint_state[0])  # Position\n",
    "            joint_velocities.append(joint_state[1])  # Velocity\n",
    "        # Removed detailed logging to reduce overhead\n",
    "        return np.array(joint_positions, dtype=np.float32), np.array(joint_velocities, dtype=np.float32)\n",
    "\n",
    "    def apply_action(self, delta_deg: np.ndarray, target_positions: np.ndarray):\n",
    "        kp = [1.2, 0.8, 0.8, 0.8, 0.8, 0.8]  # PD controller proportional gains\n",
    "        kd = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]  # PD controller derivative gains\n",
    "\n",
    "        dt = 1/15  # Time step\n",
    "        alpha = 0.07  # Low-pass filter coefficient (adjust as needed)\n",
    "\n",
    "        # Apply the low-pass filter to the RL actions (delta_deg)\n",
    "        filtered_delta_deg = alpha * delta_deg + (1 - alpha) * self.filtered_action\n",
    "        self.filtered_action = filtered_delta_deg\n",
    "\n",
    "        for i in range(self.n_joints):\n",
    "            current_pos, _ = p.getJointState(self.robot_id, self.joint_indices[i])[:2]\n",
    "            # Calculate errors\n",
    "            position_error = target_positions[i] - current_pos + filtered_delta_deg[i]\n",
    "            d_error = (position_error - self.prev_position_error[i]) / dt\n",
    "            # PD control\n",
    "            pd_output_velocity = (kp[i] * position_error) + (kd[i] * d_error)\n",
    "            pd_output_velocity = np.clip(pd_output_velocity, -2, 2)  # Limit velocities\n",
    "\n",
    "            # Optional: Zero out control for specific joints if needed\n",
    "            if i in [0, 4, 5]:\n",
    "                pd_output_velocity = 0\n",
    "\n",
    "            p.setJointMotorControl2(\n",
    "                bodyIndex=self.robot_id,\n",
    "                jointIndex=self.joint_indices[i],\n",
    "                controlMode=p.VELOCITY_CONTROL,\n",
    "                targetVelocity=pd_output_velocity,\n",
    "                force=500\n",
    "            )\n",
    "            self.prev_position_error[i] = position_error\n",
    "            self.delta_deg_prev[i] = delta_deg[i]\n",
    "\n",
    "    def disconnect(self):\n",
    "        \"\"\"\n",
    "        Disconnects the robot from the simulation.\n",
    "        \"\"\"\n",
    "        if self.robot_id is not None:\n",
    "            p.removeBody(self.robot_id)\n",
    "            logger.info(f\"Robot with ID {self.robot_id} removed from simulation.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Multi-Agent Environment\n",
    "# ----------------------------\n",
    "class MultiAgentUR5eEnv(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Multi-agent Gymnasium environment for the UR5e robotic arm.\n",
    "    Each joint is controlled by a separate agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, render_mode=None, max_step=600):  # Reduced max_step from 12000 to 2000\n",
    "        super().__init__()\n",
    "        self.robot = CustomUR5eRobot(urdf_path=\"ur5e.urdf\")  # Ensure ur5e.urdf is in the correct path\n",
    "        self.n_agents = self.robot.n_joints\n",
    "        self.agents = [f\"agent_{i}\" for i in range(self.n_agents)]\n",
    "        # Action space: single float per agent representing target velocity\n",
    "        self.action_space = spaces.Box(low=-0.1, high=0.1, shape=(1,), dtype=np.float32)\n",
    "        # Observation space: (target_position - current_position)\n",
    "        self.observation_space = spaces.Box(low=np.array([-2*np.pi, -6]),\n",
    "                                            high=np.array([2*np.pi, 6]),\n",
    "                                            dtype=np.float32)\n",
    "        self.render_mode = render_mode\n",
    "        self.max_steps = max_step\n",
    "        self.current_step = 0\n",
    "        self.target_positions = self.robot.invKine([-0.5, -0.5  , 0.0 ])  # Target joint positions\n",
    "        self.target_velocity = np.zeros(6)\n",
    "\n",
    "\n",
    "        self.dones = {agent: False for agent in self.agents}\n",
    "\n",
    "        # Initialize PyBullet physics client\n",
    "        if self.render_mode == \"human\":\n",
    "            self.physics_client = p.connect(p.GUI)\n",
    "            logger.info(\"PyBullet GUI mode enabled.\")\n",
    "        else:\n",
    "            self.physics_client = p.connect(p.DIRECT)\n",
    "            logger.info(\"PyBullet DIRECT mode enabled.\")\n",
    "\n",
    "        # Additional setup\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        p.setGravity(0, 0, 0)  # Set gravity\n",
    "        self.robot.load_robot()\n",
    "        self.robot.reset_joints(initial_positions=self.robot.invKine([-0.5, -0.5  , 0.0 ]),\n",
    "                                initial_velocities=np.zeros(self.n_agents))\n",
    "    def get_target_position(self, step):\n",
    "        \"\"\"\n",
    "        Returns the desired end-effector position at the given step.\n",
    "        For example, a circular trajectory in the XY-plane.\n",
    "        \"\"\"\n",
    "        # Parameters for the circular trajectory\n",
    "        radius = 0.1  # Adjust as needed\n",
    "        angular_speed = 0.001  # Adjust as needed\n",
    "\n",
    "        # Calculate the angle for the current step\n",
    "        angle = angular_speed * step\n",
    "        xyz_init = [-0.5, -0.5  , 0.0 ]\n",
    "        xyz = [-0.5, -0.5  , 0.5 ]\n",
    "        # Desired X, Y, Z position\n",
    "        dt1 = 15 \n",
    "\n",
    "        # x_desired = xyz_init[0] # + radius * np.cos(angle)\n",
    "        # y_desired = xyz_init[1] # + radius * np.sin(angle)\n",
    "        # z_desired = xyz_init[2] + radius * np.cos(angle)\n",
    "\n",
    "\n",
    "        if step >= dt1 * 5 and step <= dt1 * 25 :\n",
    "            x_desired = xyz[0] # + radius * np.cos(angle)\n",
    "            y_desired = xyz[1] # + radius * np.sin(angle)\n",
    "            z_desired = xyz[2] # Keep Z constant, or define a function\n",
    "        else:\n",
    "            x_desired = xyz_init[0] # + radius * np.cos(angle)\n",
    "            y_desired = xyz_init[1] # + radius * np.sin(angle)\n",
    "            z_desired = xyz_init[2]  # Keep Z constant, or define a function \n",
    "            \n",
    "        return np.array([x_desired, y_desired, z_desired])\n",
    "    \n",
    "    \n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[Dict[str, Any]] = None) -> Tuple[Dict[str, np.ndarray], Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Resets the environment to an initial state and returns an initial observation.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)  # Example: Seed NumPy's RNG\n",
    "            # PyBullet does not support direct seeding, but you can manage randomness as needed\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.dones = {agent: False for agent in self.agents}\n",
    "\n",
    "        # Reset the simulation\n",
    "        p.resetSimulation()\n",
    "        p.setGravity(0, 0, 0)\n",
    "        p.setTimeStep(1./15.)\n",
    "\n",
    "        # Load the robot model into the simulation\n",
    "        self.robot.load_robot()\n",
    "\n",
    "        # Initialize joint positions and velocities\n",
    "        initial_positions = self.robot.invKine([-0.5, -0.5  , 0.0 ]) \n",
    "        initial_velocities = np.zeros(self.n_agents)\n",
    "        self.robot.reset_joints(initial_positions, initial_velocities)\n",
    "\n",
    "        observations, _ = self._get_obs()\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "        return observations, infos\n",
    "\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        \"\"\"\n",
    "        Executes a step in the environment given the actions of all agents.\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Collect actions\n",
    "        delta_deg = np.zeros(self.n_agents)\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            delta_deg[i] = action_dict[agent][0]\n",
    "\n",
    "        # Store actions for reward computation\n",
    "        self.last_actions = action_dict\n",
    "\n",
    "        # Apply actions to the robot using PD control\n",
    "        self.target_end_effector_pos = self.get_target_position(self.current_step)\n",
    "        self.target_positions = self.robot.invKine(self.target_end_effector_pos)\n",
    "        self.robot.apply_action(delta_deg, self.target_positions)\n",
    "        # Advance the simulation by one time step\n",
    "        p.stepSimulation()\n",
    "\n",
    "        # Get observations (positions and velocities)\n",
    "        joint_positions, joint_velocities = self.robot.get_observation()\n",
    "\n",
    "        # Compute X, Y, Z positions using Forward Kinematics\n",
    "        x, y, z = self.robot.FK(joint_positions)\n",
    "\n",
    "        # Compute individual end-effector errors\n",
    "        x_error = x - self.target_end_effector_pos[0]\n",
    "        y_error = y - self.target_end_effector_pos[1]\n",
    "        z_error = z - self.target_end_effector_pos[2]\n",
    "\n",
    "        # Compute total end-effector error\n",
    "        end_effector_error = np.linalg.norm([x_error, y_error, z_error])\n",
    "        self.end_effector_error = end_effector_error\n",
    "\n",
    "        # Check if target is reached\n",
    "        target_threshold = 0.001  # Adjust threshold as needed\n",
    "        if not hasattr(self, 'target_reached'):\n",
    "            self.target_reached = False\n",
    "\n",
    "        if not self.target_reached and end_effector_error <= target_threshold:\n",
    "            self.target_reached = True\n",
    "            # Store the time step when the target was first reached\n",
    "            self.target_reach_step = self.current_step\n",
    "\n",
    "        # Clamp joint positions to be within observation space bounds\n",
    "        joint_positions_clamped = np.clip(joint_positions, -np.pi, np.pi)\n",
    "        joint_velocities_clamped = np.clip(joint_velocities, -2, 2)\n",
    "\n",
    "        # Compute position errors for observations\n",
    "        position_errors = self.target_positions - joint_positions_clamped\n",
    "        # velocity_errors = self.target_velocity - joint_velocities_clamped\n",
    "\n",
    "        observations = {}\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            observations[agent] = np.array([joint_positions_clamped[i], joint_velocities_clamped[i]], dtype=np.float32)\n",
    "\n",
    "        # Calculate rewards\n",
    "        rewards = self._compute_rewards(observations)\n",
    "\n",
    "        # Update infos to include target positions and errors\n",
    "        infos = {\n",
    "            agent: {\n",
    "                \"joint_position\": joint_positions_clamped[i],\n",
    "                \"joint_velocity\": joint_velocities_clamped[i],\n",
    "                \"reward\": rewards[agent],\n",
    "                \"end_effector_x\": x,\n",
    "                \"end_effector_y\": y,\n",
    "                \"end_effector_z\": z,\n",
    "                \"target_end_effector_x\": self.target_end_effector_pos[0],\n",
    "                \"target_end_effector_y\": self.target_end_effector_pos[1],\n",
    "                \"target_end_effector_z\": self.target_end_effector_pos[2],\n",
    "                \"x_error\": x_error,\n",
    "                \"y_error\": y_error,\n",
    "                \"z_error\": z_error,\n",
    "                \"end_effector_error\": end_effector_error,\n",
    "            } for i, agent in enumerate(self.agents)\n",
    "        }\n",
    "\n",
    "        # Check termination conditions\n",
    "        done = self.current_step >= self.max_steps\n",
    "\n",
    "        # Build the \"terminations\" and \"truncations\" flag dicts for all agents\n",
    "        terminations = {agent: False for agent in self.agents}\n",
    "        truncations = {agent: done for agent in self.agents}\n",
    "        terminations[\"__all__\"] = False\n",
    "        truncations[\"__all__\"] = done\n",
    "\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Retrieves the current joint positions and calculates the difference with target positions.\n",
    "        \"\"\"\n",
    "        joint_positions, joint_velocities = self.robot.get_observation()\n",
    "        position_errors = self.target_positions - joint_positions\n",
    "        velocity_errors = self.target_velocity - joint_velocities\n",
    "        observations = {}\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            observations[agent] = np.array([joint_positions[i], joint_velocities[i]], dtype=np.float32)\n",
    "        return observations, {}\n",
    "    \n",
    "\n",
    "    def _compute_rewards(self, observations):\n",
    "        rewards = {}\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            # Penalize the end-effector error at each time step\n",
    "            joint_position_error = abs(self.target_positions[i] - observations[agent][0])\n",
    "            joint_position_error = -joint_position_error  # The closer to the target, the less negative\n",
    "            dt1 = 15\n",
    "            joint_velocity_error = -abs(0 - observations[agent][1])\n",
    "            # Include a time penalty that increases over time\n",
    "            if self.current_step <= dt1 * 5:\n",
    "                time = 1\n",
    "            elif self.current_step >= dt1 * 5 and self.current_step <= dt1 * 10:\n",
    "                time = self.current_step - (dt1 * 5)\n",
    "                time = (dt1 * 20)/(time + 10)\n",
    "            elif self.current_step > dt1 * 25 and self.current_step <= dt1 * 30:\n",
    "                time = self.current_step - (dt1 * 25)\n",
    "                time = (dt1 * 15)/(time + 10)\n",
    "            else:\n",
    "                time = 1\n",
    "\n",
    "            time_penalty = time  # Adjust the coefficient as needed\n",
    "            reward = joint_position_error# * time_penalty\n",
    "\n",
    "            # Assign the computed reward to the agent\n",
    "            rewards[agent] = reward\n",
    "\n",
    "        return rewards\n",
    "\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Renders the environment. Rendering is handled by PyBullet.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Closes the environment and disconnects PyBullet.\n",
    "        \"\"\"\n",
    "        p.disconnect()\n",
    "        self.robot.disconnect()\n",
    "        logger.info(\"PyBullet simulation disconnected.\")\n",
    "\n",
    "\n",
    "\n",
    "class JointPositionLogger(DefaultCallbacks):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log_dir = \"joint_logs_PPO\"  # Default log directory\n",
    "        self.episode_counter = 1  # Initialize episode counter\n",
    "        self.total_rewards_per_episode = {}  # Dict to store rewards per agent\n",
    "\n",
    "    def on_algorithm_init(self, *, algorithm, **kwargs):\n",
    "        self.log_dir = algorithm.config.get(\"callbacks_config\", {}).get(\"log_dir\", \"joint_logs_PPO\")\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        logger.info(f\"JointPositionLogger initialized with log directory: {self.log_dir}\")\n",
    "\n",
    "    def on_episode_start(self, *, worker, base_env, policies, episode, **kwargs):\n",
    "        sequential_episode_id = self.episode_counter\n",
    "        self.episode_counter += 1\n",
    "        episode.user_data[\"sequential_episode_id\"] = sequential_episode_id\n",
    "\n",
    "        file_path = os.path.join(self.log_dir, f\"episode_{sequential_episode_id}.csv\")\n",
    "        os.makedirs(self.log_dir, exist_ok=True)  # Ensure the log directory exists\n",
    "        csv_file = open(file_path, mode='w', newline='')\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=[\n",
    "            \"step\", \"agent_id\", \"joint_position\", \"joint_velocity\",\n",
    "            \"end_effector_x\", \"end_effector_y\", \"end_effector_z\",\n",
    "            \"target_end_effector_x\", \"target_end_effector_y\", \"target_end_effector_z\",\n",
    "            \"x_error\", \"y_error\", \"z_error\",\n",
    "            \"end_effector_error\",\n",
    "            \"reward\"\n",
    "        ])\n",
    "        writer.writeheader()\n",
    "        episode.user_data[\"csv_file\"] = csv_file\n",
    "        episode.user_data[\"csv_writer\"] = writer\n",
    "        episode.user_data[\"episode_rewards\"] = {}\n",
    "\n",
    "    def on_episode_step(self, *, worker, base_env, policies, episode, **kwargs):\n",
    "        writer = episode.user_data.get(\"csv_writer\")\n",
    "        if writer is None:\n",
    "            return  # Skip logging if writer is not initialized\n",
    "\n",
    "        for agent_id in episode.get_agents():\n",
    "            try:\n",
    "                info = episode.last_info_for(agent_id) or {}\n",
    "            except AttributeError:\n",
    "                info = episode._last_infos.get(agent_id, {})\n",
    "            if info is None:\n",
    "                continue\n",
    "\n",
    "            joint_pos = info.get(\"joint_position\", 0.0)\n",
    "            joint_vel = info.get(\"joint_velocity\", 0.0)\n",
    "            end_effector_x = info.get(\"end_effector_x\", 0.0)\n",
    "            end_effector_y = info.get(\"end_effector_y\", 0.0)\n",
    "            end_effector_z = info.get(\"end_effector_z\", 0.0)\n",
    "            target_end_effector_x = info.get(\"target_end_effector_x\", 0.0)\n",
    "            target_end_effector_y = info.get(\"target_end_effector_y\", 0.0)\n",
    "            target_end_effector_z = info.get(\"target_end_effector_z\", 0.0)\n",
    "            x_error = info.get(\"x_error\", 0.0)\n",
    "            y_error = info.get(\"y_error\", 0.0)\n",
    "            z_error = info.get(\"z_error\", 0.0)\n",
    "            end_effector_error = info.get(\"end_effector_error\", 0.0)\n",
    "            per_step_reward = info.get(\"reward\", 0.0)  # Get reward from info\n",
    "\n",
    "            # Accumulate rewards safely\n",
    "            episode.user_data.setdefault(\"episode_rewards\", {}).setdefault(agent_id, 0.0)\n",
    "            episode.user_data[\"episode_rewards\"][agent_id] += per_step_reward\n",
    "\n",
    "            # Log data\n",
    "            log_entry = {\n",
    "                \"step\": episode.length,\n",
    "                \"agent_id\": agent_id,\n",
    "                \"joint_position\": joint_pos,\n",
    "                \"joint_velocity\": joint_vel,\n",
    "                \"end_effector_x\": end_effector_x,\n",
    "                \"end_effector_y\": end_effector_y,\n",
    "                \"end_effector_z\": end_effector_z,\n",
    "                \"target_end_effector_x\": target_end_effector_x,\n",
    "                \"target_end_effector_y\": target_end_effector_y,\n",
    "                \"target_end_effector_z\": target_end_effector_z,\n",
    "                \"x_error\": x_error,\n",
    "                \"y_error\": y_error,\n",
    "                \"z_error\": z_error,\n",
    "                \"end_effector_error\": end_effector_error,\n",
    "                \"reward\": per_step_reward\n",
    "            }\n",
    "            writer.writerow(log_entry)\n",
    "\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, **kwargs):\n",
    "        csv_file = episode.user_data.get(\"csv_file\")\n",
    "        sequential_episode_id = episode.user_data.get(\"sequential_episode_id\")\n",
    "        if csv_file:\n",
    "            csv_file.close()\n",
    "            logger.info(\n",
    "                f\"Ended Episode {sequential_episode_id} | Log saved to \"\n",
    "                f\"{os.path.join(self.log_dir, f'episode_{sequential_episode_id}.csv')}\"\n",
    "            )\n",
    "\n",
    "            # Generate per-episode plots\n",
    "            self.generate_joint_position_plot(sequential_episode_id)\n",
    "            self.generate_joint_velocity_plot(sequential_episode_id)\n",
    "            self.generate_end_effector_plot(sequential_episode_id)\n",
    "            self.generate_reward_plot(sequential_episode_id)\n",
    "            self.generate_xyz_error_plot(sequential_episode_id)\n",
    "\n",
    "        # Calculate total reward per agent\n",
    "        episode_rewards = episode.user_data.get(\"episode_rewards\", {})\n",
    "        for agent_id, total_reward in episode_rewards.items():\n",
    "            self.total_rewards_per_episode.setdefault(agent_id, []).append(total_reward)\n",
    "\n",
    "        # Generate the plot of total rewards per episode per agent\n",
    "        self.generate_total_reward_plot()\n",
    "\n",
    "    def generate_total_reward_plot(self):\n",
    "        plt.figure()\n",
    "        # Check if there are any rewards recorded\n",
    "        if not self.total_rewards_per_episode:\n",
    "            logger.warning(\"No rewards to plot.\")\n",
    "            return\n",
    "\n",
    "        num_episodes = len(next(iter(self.total_rewards_per_episode.values())))\n",
    "        episodes = list(range(1, num_episodes + 1))\n",
    "\n",
    "        for agent_id, rewards in self.total_rewards_per_episode.items():\n",
    "            plt.plot(episodes, rewards, label=agent_id)\n",
    "\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Total Reward per Episode per Agent')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plot_filename = 'total_rewards_per_episode.png'\n",
    "        plot_path = os.path.join(self.log_dir, plot_filename)\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        logger.info(f\"Saved total rewards plot to {plot_path}\")\n",
    "\n",
    "\n",
    "    def generate_xyz_error_plot(self, episode_id: int):\n",
    "        \"\"\"\n",
    "        Generates and saves plots for x_error, y_error, and z_error over time with x-axis in seconds.\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(self.log_dir, f\"episode_{episode_id}.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"CSV file for Episode {episode_id} does not exist.\")\n",
    "            return\n",
    "\n",
    "        steps = []\n",
    "        x_errors = []\n",
    "        y_errors = []\n",
    "        z_errors = []\n",
    "\n",
    "        # Read data from CSV\n",
    "        try:\n",
    "            with open(file_path, mode='r') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "                for row in reader:\n",
    "                    if row[\"agent_id\"] != \"agent_0\":\n",
    "                        continue  # Avoid duplicating entries for each agent\n",
    "\n",
    "                    step = int(row[\"step\"])\n",
    "                    x_error = float(row[\"x_error\"])\n",
    "                    y_error = float(row[\"y_error\"])\n",
    "                    z_error = float(row[\"z_error\"])\n",
    "\n",
    "                    steps.append(step)\n",
    "                    x_errors.append(x_error)\n",
    "                    y_errors.append(y_error)\n",
    "                    z_errors.append(z_error)\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to read CSV for Episode {episode_id}: {e}\")\n",
    "            return\n",
    "\n",
    "        # Convert steps to seconds\n",
    "        seconds = steps_to_seconds(steps)\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(seconds, x_errors, label=\"X Error\")\n",
    "        plt.plot(seconds, y_errors, label=\"Y Error\")\n",
    "        plt.plot(seconds, z_errors, label=\"Z Error\")\n",
    "\n",
    "        plt.xlabel(\"Seconds\")  # Updated label\n",
    "        plt.ylabel(\"Error (meters)\")\n",
    "        plt.title(f\"End-Effector Errors for Episode {episode_id}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot\n",
    "        plot_filename = f\"episode_{episode_id}_xyz_errors.png\"\n",
    "        plot_path = os.path.join(self.log_dir, plot_filename)\n",
    "        try:\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "            logger.info(f\"Saved end-effector error plot for Episode {episode_id} to {plot_path}\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to save end-effector error plot for Episode {episode_id}: {e}\")\n",
    "\n",
    "\n",
    "    def generate_joint_position_plot(self, episode_id: int):\n",
    "        \"\"\"\n",
    "        Generates and saves a joint position plot for the specified episode with x-axis in seconds.\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(self.log_dir, f\"episode_{episode_id}.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"CSV file for Episode {episode_id} does not exist.\")\n",
    "            return\n",
    "\n",
    "        agent_data = {}\n",
    "        steps = {}\n",
    "\n",
    "        # Read data from CSV\n",
    "        try:\n",
    "            with open(file_path, mode='r') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "                for row in reader:\n",
    "                    step = int(row[\"step\"])\n",
    "                    agent_id = row[\"agent_id\"]\n",
    "                    joint_pos = float(row[\"joint_position\"])\n",
    "\n",
    "                    if agent_id not in agent_data:\n",
    "                        agent_data[agent_id] = []\n",
    "                        steps[agent_id] = []\n",
    "\n",
    "                    steps[agent_id].append(step)\n",
    "                    agent_data[agent_id].append(joint_pos)\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to read CSV for Episode {episode_id}: {e}\")\n",
    "            return\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for agent_id in agent_data:\n",
    "            agent_steps = steps[agent_id]\n",
    "            agent_seconds = steps_to_seconds(agent_steps)\n",
    "            agent_positions = agent_data[agent_id]\n",
    "            plt.plot(agent_seconds, agent_positions, label=agent_id)\n",
    "\n",
    "        plt.xlabel(\"Seconds\")  # Updated label\n",
    "        plt.ylabel(\"Joint Position (radians)\")\n",
    "        plt.title(f\"Joint Positions for Episode {episode_id}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot\n",
    "        plot_filename = f\"episode_{episode_id}_joint_positions.png\"\n",
    "        plot_path = os.path.join(self.log_dir, plot_filename)\n",
    "        try:\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "            logger.info(f\"Saved joint positions plot for Episode {episode_id} to {plot_path}\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to save joint positions plot for Episode {episode_id}: {e}\")\n",
    "\n",
    "\n",
    "    def generate_joint_velocity_plot(self, episode_id: int):\n",
    "        \"\"\"\n",
    "        Generates and saves a joint velocity plot for the specified episode with x-axis in seconds.\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(self.log_dir, f\"episode_{episode_id}.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"CSV file for Episode {episode_id} does not exist.\")\n",
    "            return\n",
    "\n",
    "        agent_data = {}\n",
    "        steps = {}\n",
    "\n",
    "        # Read data from CSV\n",
    "        try:\n",
    "            with open(file_path, mode='r') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "                for row in reader:\n",
    "                    step = int(row[\"step\"])\n",
    "                    agent_id = row[\"agent_id\"]\n",
    "                    joint_vel = float(row[\"joint_velocity\"])\n",
    "\n",
    "                    if agent_id not in agent_data:\n",
    "                        agent_data[agent_id] = []\n",
    "                        steps[agent_id] = []\n",
    "\n",
    "                    steps[agent_id].append(step)\n",
    "                    agent_data[agent_id].append(joint_vel)\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to read CSV for Episode {episode_id}: {e}\")\n",
    "            return\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for agent_id in agent_data:\n",
    "            agent_steps = steps[agent_id]\n",
    "            agent_seconds = steps_to_seconds(agent_steps)\n",
    "            agent_velocities = agent_data[agent_id]\n",
    "            plt.plot(agent_seconds, agent_velocities, label=agent_id)\n",
    "\n",
    "        plt.xlabel(\"Seconds\")  # Updated label\n",
    "        plt.ylabel(\"Joint Velocity (radians per second)\")\n",
    "        plt.title(f\"Joint Velocities for Episode {episode_id}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot\n",
    "        plot_filename = f\"episode_{episode_id}_joint_velocities.png\"\n",
    "        plot_path = os.path.join(self.log_dir, plot_filename)\n",
    "        try:\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "            logger.info(f\"Saved joint velocities plot for Episode {episode_id} to {plot_path}\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to save joint velocities plot for Episode {episode_id}: {e}\")\n",
    "\n",
    "    def generate_end_effector_plot(self, episode_id: int):\n",
    "        \"\"\"\n",
    "        Generates and saves an end-effector position plot (X, Y, Z) for the specified episode with x-axis in seconds.\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(self.log_dir, f\"episode_{episode_id}.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"CSV file for Episode {episode_id} does not exist.\")\n",
    "            return\n",
    "\n",
    "        # Initialize data structures\n",
    "        steps = []\n",
    "        x_positions = []\n",
    "        y_positions = []\n",
    "        z_positions = []\n",
    "        target_x_positions = []\n",
    "        target_y_positions = []\n",
    "        target_z_positions = []\n",
    "\n",
    "        try:\n",
    "            with open(file_path, mode='r') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "                for row in reader:\n",
    "                    step = int(row[\"step\"])\n",
    "                    x = float(row[\"end_effector_x\"])\n",
    "                    y = float(row[\"end_effector_y\"])\n",
    "                    z = float(row[\"end_effector_z\"])\n",
    "                    target_x = float(row[\"target_end_effector_x\"])\n",
    "                    target_y = float(row[\"target_end_effector_y\"])\n",
    "                    target_z = float(row[\"target_end_effector_z\"])\n",
    "\n",
    "                    steps.append(step)\n",
    "                    x_positions.append(x)\n",
    "                    y_positions.append(y)\n",
    "                    z_positions.append(z)\n",
    "                    target_x_positions.append(target_x)\n",
    "                    target_y_positions.append(target_y)\n",
    "                    target_z_positions.append(target_z)\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to read CSV for Episode {episode_id}: {e}\")\n",
    "            return\n",
    "\n",
    "        # Convert steps to seconds\n",
    "        seconds = steps_to_seconds(steps)\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(seconds, x_positions, label=\"Actual X Position\")\n",
    "        plt.plot(seconds, target_x_positions, label=\"Target X Position\", linestyle='--')\n",
    "\n",
    "        plt.plot(seconds, y_positions, label=\"Actual Y Position\")\n",
    "        plt.plot(seconds, target_y_positions, label=\"Target Y Position\", linestyle='--')\n",
    "\n",
    "        plt.plot(seconds, z_positions, label=\"Actual Z Position\")\n",
    "        plt.plot(seconds, target_z_positions, label=\"Target Z Position\", linestyle='--')\n",
    "\n",
    "        plt.xlabel(\"Seconds\")  # Updated label\n",
    "        plt.ylabel(\"Position (meters)\")\n",
    "        plt.title(f\"End-Effector Positions for Episode {episode_id}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot\n",
    "        plot_filename = f\"episode_{episode_id}_end_effector_positions.png\"\n",
    "        plot_path = os.path.join(self.log_dir, plot_filename)\n",
    "        try:\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "            logger.info(f\"Saved end-effector positions plot for Episode {episode_id} to {plot_path}\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to save end-effector plot for Episode {episode_id}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    def generate_reward_plot(self, episode_id: int):\n",
    "        \"\"\"\n",
    "        Generates and saves a reward plot for the specified episode with x-axis in seconds.\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(self.log_dir, f\"episode_{episode_id}.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"CSV file for Episode {episode_id} does not exist.\")\n",
    "            return\n",
    "\n",
    "        agent_rewards = {}\n",
    "        steps = []\n",
    "\n",
    "        # Read data from CSV\n",
    "        try:\n",
    "            with open(file_path, mode='r') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "                for row in reader:\n",
    "                    step = int(row[\"step\"])\n",
    "                    agent_id = row[\"agent_id\"]\n",
    "                    reward = float(row[\"reward\"])\n",
    "\n",
    "                    if agent_id not in agent_rewards:\n",
    "                        agent_rewards[agent_id] = []\n",
    "\n",
    "                    agent_rewards[agent_id].append(reward)\n",
    "                    if step not in steps:\n",
    "                        steps.append(step)\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to read CSV for Episode {episode_id}: {e}\")\n",
    "            return\n",
    "\n",
    "        # Convert steps to seconds\n",
    "        seconds = steps_to_seconds(steps)\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for agent_id, rewards in agent_rewards.items():\n",
    "            plt.plot(seconds, rewards, label=agent_id)\n",
    "\n",
    "        plt.xlabel(\"Seconds\")  # Updated label\n",
    "        plt.ylabel(\"Per-Step Reward\")\n",
    "        plt.title(f\"Per-Step Rewards for Episode {episode_id}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot\n",
    "        plot_filename = f\"episode_{episode_id}_rewards.png\"\n",
    "        plot_path = os.path.join(self.log_dir, plot_filename)\n",
    "        try:\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "            logger.info(f\"Saved reward plot for Episode {episode_id} to {plot_path}\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to save reward plot for Episode {episode_id}: {e}\")\n",
    "\n",
    "\n",
    "    def generate_error_plot(self, episode_id: int):\n",
    "        \"\"\"\n",
    "        Generates and saves an end-effector error plot for the specified episode.\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(self.log_dir, f\"episode_{episode_id}.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"CSV file for Episode {episode_id} does not exist.\")\n",
    "            return\n",
    "\n",
    "        steps = []\n",
    "        errors = []\n",
    "\n",
    "        # Read data from CSV\n",
    "        try:\n",
    "            with open(file_path, mode='r') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "                for row in reader:\n",
    "                    step = int(row[\"step\"])\n",
    "                    error = float(row[\"end_effector_error\"])\n",
    "\n",
    "                    steps.append(step)\n",
    "                    errors.append(error)\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to read CSV for Episode {episode_id}: {e}\")\n",
    "            return\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(steps, errors, label=\"End-Effector Error\")\n",
    "\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"Error (meters)\")\n",
    "        plt.title(f\"End-Effector Error for Episode {episode_id}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot\n",
    "        plot_filename = f\"episode_{episode_id}_end_effector_error.png\"\n",
    "        plot_path = os.path.join(self.log_dir, plot_filename)\n",
    "        try:\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "            logger.info(f\"Saved end-effector error plot for Episode {episode_id} to {plot_path}\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to save end-effector error plot for Episode {episode_id}: {e}\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Plotting Function for Rewards\n",
    "# ----------------------------\n",
    "def plot_rewards(log_dir: str, output_dir: str = \"reward_plots\"):\n",
    "    \"\"\"\n",
    "    Generates and saves reward plots for each episode with x-axis in seconds.\n",
    "    \n",
    "    :param log_dir: Directory where CSV logs are stored.\n",
    "    :param output_dir: Directory to save the generated reward plots.\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    logger.info(f\"Saving reward plots to directory: {output_dir}\")\n",
    "    \n",
    "    # Gather all CSV files corresponding to episodes\n",
    "    csv_files = [f for f in os.listdir(log_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    if not csv_files:\n",
    "        logger.warning(f\"No CSV files found in {log_dir} to plot rewards.\")\n",
    "        return\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        episode_id = csv_file.replace(\"episode_\", \"\").replace(\".csv\", \"\")\n",
    "        file_path = os.path.join(log_dir, csv_file)\n",
    "        \n",
    "        # Initialize data structures\n",
    "        agent_rewards = {}\n",
    "        steps = []\n",
    "        \n",
    "        # Read data from CSV\n",
    "        with open(file_path, mode='r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                step = int(row[\"step\"])\n",
    "                agent_id = row[\"agent_id\"]\n",
    "                reward = float(row[\"reward\"])\n",
    "                \n",
    "                if agent_id not in agent_rewards:\n",
    "                    agent_rewards[agent_id] = []\n",
    "                \n",
    "                agent_rewards[agent_id].append(reward)\n",
    "                if step not in steps:\n",
    "                    steps.append(step)\n",
    "        \n",
    "        # Convert steps to seconds\n",
    "        seconds = steps_to_seconds(steps)\n",
    "        \n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for agent_id, rewards in agent_rewards.items():\n",
    "            plt.plot(seconds, rewards, label=agent_id)\n",
    "        \n",
    "        plt.xlabel(\"Seconds\")  # Updated label\n",
    "        plt.ylabel(\"Per-Step Reward\")\n",
    "        plt.title(f\"Per-Step Rewards for Episode {episode_id}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_filename = f\"episode_{episode_id}_rewards.png\"\n",
    "        plot_path = os.path.join(output_dir, plot_filename)\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        logger.info(f\"Saved reward plot for Episode {episode_id} to {plot_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Plotting Average Rewards Across Episodes\n",
    "# ----------------------------\n",
    "def plot_average_rewards(log_dir: str, output_dir: str = \"average_reward_plots\"):\n",
    "    \"\"\"\n",
    "    Generates and saves average reward plots across all episodes with x-axis in seconds.\n",
    "    \n",
    "    :param log_dir: Directory where CSV logs are stored.\n",
    "    :param output_dir: Directory to save the generated plots.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    logger.info(f\"Saving average reward plots to directory: {output_dir}\")\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(log_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    if not csv_files:\n",
    "        logger.warning(f\"No CSV files found in {log_dir} to plot average rewards.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize data structures\n",
    "    agent_rewards_over_episodes = {f\"agent_{i}\": [] for i in range(6)}\n",
    "    max_steps = 0\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(log_dir, csv_file)\n",
    "        with open(file_path, mode='r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            episode_rewards = {agent_id: [] for agent_id in agent_rewards_over_episodes.keys()}\n",
    "            for row in reader:\n",
    "                agent_id = row[\"agent_id\"]\n",
    "                reward = float(row[\"reward\"])\n",
    "                episode_rewards[agent_id].append(reward)\n",
    "        \n",
    "        for agent_id, rewards in episode_rewards.items():\n",
    "            agent_rewards_over_episodes[agent_id].append(rewards)\n",
    "            if len(rewards) > max_steps:\n",
    "                max_steps = len(rewards)\n",
    "    \n",
    "    # Calculate average rewards per step for each agent\n",
    "    average_rewards = {}\n",
    "    for agent_id, rewards_list in agent_rewards_over_episodes.items():\n",
    "        # Pad shorter episodes with np.nan for averaging\n",
    "        padded_rewards = np.full((len(rewards_list), max_steps), np.nan)\n",
    "        for i, rewards in enumerate(rewards_list):\n",
    "            padded_rewards[i, :len(rewards)] = rewards\n",
    "        average_rewards[agent_id] = np.nanmean(padded_rewards, axis=0)\n",
    "    \n",
    "    # Convert steps to seconds\n",
    "    seconds = steps_to_seconds(range(1, max_steps + 1))\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for agent_id, avg_rewards in average_rewards.items():\n",
    "        plt.plot(seconds, avg_rewards, label=agent_id)\n",
    "    \n",
    "    plt.xlabel(\"Seconds\")  # Updated label\n",
    "    plt.ylabel(\"Average Per-Step Reward\")\n",
    "    plt.title(\"Average Per-Step Rewards Across Episodes\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_filename = \"average_rewards_across_episodes.png\"\n",
    "    plot_path = os.path.join(output_dir, plot_filename)\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    logger.info(f\"Saved average reward plot to {plot_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Policy Mapping Function\n",
    "# ----------------------------\n",
    "def policy_mapping_fn(agent_id, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Maps each agent to its corresponding policy based on agent ID.\n",
    "    \"\"\"\n",
    "    return agent_id\n",
    "\n",
    "\n",
    "\n",
    "def steps_to_seconds(steps, steps_per_second=15):\n",
    "    \"\"\"\n",
    "    Converts step counts to seconds.\n",
    "    \n",
    "    :param steps: List or array of step numbers.\n",
    "    :param steps_per_second: Number of steps that correspond to one second.\n",
    "    :return: List or array of time in seconds.\n",
    "    \"\"\"\n",
    "    return np.array(steps) / steps_per_second\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main Training Function\n",
    "# ----------------------------\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to initialize Ray, configure the PPO trainer, and execute the training loop.\n",
    "    \"\"\"\n",
    "    # Initialize Ray if not already initialized\n",
    "    if not ray.is_initialized():\n",
    "        ray.init(ignore_reinit_error=True)\n",
    "        logger.info(\"Ray initialized.\")\n",
    "\n",
    "    config = {\n",
    "            \"env\": MultiAgentUR5eEnv,\n",
    "            \"env_config\": {},\n",
    "            \"multiagent\": {\n",
    "                \"policies\": {\n",
    "                    f\"agent_{i}\": (\n",
    "                        None,\n",
    "                        spaces.Box(low=np.array([-2*np.pi, -6]), high=np.array([2*np.pi, 6]), dtype=np.float32),  # Observation space\n",
    "                        spaces.Box(low=-0.1, high=0.1, shape=(1,), dtype=np.float32),  # Action space (delta positions)\n",
    "                        {}\n",
    "                    ) for i in range(6)\n",
    "                },\n",
    "                \"policy_mapping_fn\": policy_mapping_fn,\n",
    "                \"replay_mode\": \"independent\",\n",
    "            },\n",
    "            \"framework\": \"torch\",  # or \"tf\"\n",
    "            \"num_workers\": 1,\n",
    "            \"callbacks\": JointPositionLogger,\n",
    "            \"callbacks_config\": {\n",
    "                \"log_dir\": \"joint_logs_PPO\",\n",
    "            },\n",
    "            # SAC-specific configurations\n",
    "            \"lr\": 4e-5,\n",
    "            \"train_batch_size\": 1000,\n",
    "            \"gamma\": 0.9,\n",
    "            \"tau\": 0.005,\n",
    "            \"target_entropy\": \"auto\",\n",
    "            \"n_step\": 1,\n",
    "            \"no_done_at_end\": True,\n",
    "            \"prioritized_replay\": False,\n",
    "            \"optimization\": {\n",
    "                \"actor_learning_rate\": 4e-5,\n",
    "                \"critic_learning_rate\": 4e-5,\n",
    "                \"entropy_learning_rate\": 4e-5,\n",
    "            },\n",
    "            \"model\": {\n",
    "                \"fcnet_hiddens\": [128, 128],\n",
    "                \"fcnet_activation\": \"tanh\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = PPO(config=config)\n",
    "    logger.info(\"PPO trainer initialized.\")\n",
    "\n",
    "    # Training loop parameters\n",
    "    total_timesteps = 6_000_000  # Reduced total timesteps\n",
    "    checkpoint_interval = 20_000  # Save checkpoint every 20,000 timesteps\n",
    "\n",
    "    # Training loop\n",
    "    logger.info(f\"Starting training for {total_timesteps} timesteps...\")\n",
    "    timesteps = 0\n",
    "    try:\n",
    "        while timesteps < total_timesteps:\n",
    "            result = trainer.train()\n",
    "            timesteps = result[\"timesteps_total\"]\n",
    "            logger.info(f\"Completed {timesteps} timesteps\")\n",
    "\n",
    "            # Save checSACkpoint at intervals\n",
    "            if timesteps % checkpoint_interval == 0:\n",
    "                checkpoint_path = trainer.save()\n",
    "                logger.info(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "                \n",
    "                # Optionally, plot average rewards across episodes\n",
    "                plot_average_rewards(\n",
    "                    log_dir=\"joint_logs_PPO\", \n",
    "                    output_dir=\"average_reward_plots\"\n",
    "                )\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Training interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during training: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        logger.info(\"Training completed or interrupted. Cleaning up.\")\n",
    "        trainer.cleanup()\n",
    "        ray.shutdown()\n",
    "        logger.info(\"Ray shutdown.\")\n",
    "\n",
    "        # Optionally, generate a final average reward plot\n",
    "        plot_average_rewards(\n",
    "            log_dir=\"joint_logs_PPO\", \n",
    "            output_dir=\"average_reward_plots\"\n",
    "        )\n",
    "# Execute the Training\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
